{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_or_down(centroid, line, line_direction, threshold):\n",
    "    '''\n",
    "    centroid: the center of object\n",
    "    kind_of_line: we choose our direction\n",
    "    line: the thresh for centroid\n",
    "    threshold: a maximum \n",
    "    '''\n",
    "    if line_direction == 'horizon':\n",
    "        point = centroid[0]\n",
    "        if round(point-line) <= 5:\n",
    "            return 'up'\n",
    "    elif line_direction == 'vectorize':\n",
    "        point = centroid[1]\n",
    "        if round(point-line) <= thresh\n",
    "    else:\n",
    "        raise \"Please choose your direction is 'horizon' or 'vectorize'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6ef6e5b394cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# Our operations on the frame come here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbounding_pic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;31m# Display the resulting frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-6ef6e5b394cd>\u001b[0m in \u001b[0;36mbounding_pic\u001b[1;34m(net, image, scale, size)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0msize\u001b[0m \u001b[0mof\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     '''\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mWidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mHeight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Resize picture to 416x416, because YOLO take in 416x416\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_output_layers(net):\n",
    "    '''\n",
    "    get all output layer names: with yolov3 is yolo_82, 94 and 106\n",
    "    ''' \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, confidences, x, y, x_plus_w, y_plus_h):\n",
    "    '''\n",
    "    draw a bounding box around object\n",
    "    '''\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "def bounding_pic(net, image, scale, size):\n",
    "    '''\n",
    "    net: the model reading by open-cv (maybe YoLo or SSD)\n",
    "    image: image need to bouding box\n",
    "    scale: image pixel multiply with this\n",
    "    size: a tuple contain size of image\n",
    "    '''\n",
    "    Width = image.shape[1]\n",
    "    Height = image.shape[0]\n",
    "    # Resize picture to 416x416, because YOLO take in 416x416\n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "    # set input is resized picture\n",
    "    net.setInput(blob)\n",
    "    # last layer of Yolo model\n",
    "    outs = net.forward(get_output_layers(net))\n",
    "    '''\n",
    "    Remove\n",
    "    '''\n",
    "    print(type(outs))\n",
    "    print(len(outs))\n",
    "    print(type(outs[0]))\n",
    "    print(outs[0].shape)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.6\n",
    "    # maybe our model will detect many bouding box for an object, this threshold help us take the box with equal \n",
    "    #            or higher propability\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "    '''\n",
    "    out is a 2D tensor like (number_of_objects, score_of_each_classes), with first five element in each row is special, \n",
    "    take e.g: out[0] = temp:\n",
    "        + temp[0]: x_center of that object\n",
    "        + temp[1]: y_center of that object\n",
    "        + temp[2]: width of that object\n",
    "        + temp[3]: height of that object\n",
    "        + temp[4]: unknow value\n",
    "        + from 5 to above is the score of that object to each classes => COCO have 80 class so each row contain 85 element,\n",
    "            will be 15 with CIFAR,and 1005 with IMAGENET  \n",
    "    '''\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            # get the highest score to determine its label\n",
    "            class_id = np.argmax(scores)\n",
    "            if class_id not in [0,1,2,3,7]:\n",
    "                continue\n",
    "            else:\n",
    "                # score of that object, make sure more than 50% correct label\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # scale again with w and h\n",
    "                    center_x = int(detection[0] * Width)\n",
    "                    center_y = int(detection[1] * Height)\n",
    "                    w = int(detection[2] * Width)\n",
    "                    h = int(detection[3] * Height)\n",
    "                    # remember it return x_center and y_center, not x,y, so we need to find x,y\n",
    "                    x = center_x - w / 2\n",
    "                    y = center_y - h / 2\n",
    "                    class_ids.append(class_id)\n",
    "                    confidences.append(float(confidence))\n",
    "                    boxes.append([x, y, w, h])\n",
    "\n",
    "    # detect bouding box around objects\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    # set the counting line\n",
    "    lineThickness = 2\n",
    "    x1,y1,x2,y2 = (322,580,1030,580)\n",
    "    image = cv2.line(image, (x1, y1), (x2, y2), (0,255,0), lineThickness)\n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "        draw_prediction(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "    return image\n",
    "\n",
    "# image_path = 'motorbike.jpg'\n",
    "config = 'YOLOv3-320\\\\yolov3.cfg'\n",
    "name = 'coco.names'\n",
    "weight = 'YOLOv3-320\\\\yolov3.weights'\n",
    "\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "classes = None\n",
    "\n",
    "with open(name, 'r') as f:\n",
    "    # generate all classes of COCO, bicycle ind = 1, car ind = 2 and motorbike ind = 3\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# Read the model\n",
    "net = cv2.dnn.readNet(weight, config)\n",
    "\n",
    "# take shape of image in order to scale it to 416x416, first layer of Yolo CNN\n",
    "\n",
    "scale = 0.00392\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('C:\\\\Users\\\\TobyCurtis\\\\Desktop\\\\front.avi')\n",
    "\n",
    "count = 0\n",
    "while(True):\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    thresh = 15\n",
    "    if count%thresh==0:\n",
    "        # Our operations on the frame come here\n",
    "        img = bounding_pic(net, frame, scale, (416,416))\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame',img)\n",
    "    count += 1\n",
    "    if count == thresh*30:\n",
    "        count = 0\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(99.8653, 99.8653) (199.761, 199.761)]\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread(\"front.jpg\")\n",
    "rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "track = dlib.correlation_tracker()\n",
    "(x,y,w,h) = (100,100,100,100)\n",
    "rect = dlib.rectangle(x, y, x+w, y+h)\n",
    "track.start_track(rgb, rect)\n",
    "track.update(rgb)\n",
    "pos = track.get_position()\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-458bd93f1cca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdists\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchosen_dists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m                 \u001b[0mpre_ID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_centroid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m                 \u001b[0mc_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpre_ID\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_output_layers(net):\n",
    "    '''\n",
    "    get all output layer names: with yolov3 is yolo_82, 94 and 106\n",
    "    ''' \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, confidences, x, y, x_plus_w, y_plus_h):\n",
    "    '''\n",
    "    draw a bounding box around object\n",
    "    '''\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "def find_centroid(net, image, scale, size):\n",
    "    '''\n",
    "    Input:\n",
    "        + net: the model reading by open-cv (maybe YoLo or SSD)\n",
    "        + image: image need to bouding box\n",
    "        + scale: image pixel multiply with this\n",
    "        + size: a tuple contain size of image\n",
    "    Output:\n",
    "        + A list of centroid of all objects in image\n",
    "    '''\n",
    "    Width = image.shape[1]\n",
    "    Height = image.shape[0]\n",
    "    # Resize picture to 416x416, because YOLO take in 416x416\n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "    # set input is resized picture\n",
    "    net.setInput(blob)\n",
    "    # last layer of Yolo model\n",
    "    outs = net.forward(get_output_layers(net))\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.6\n",
    "    # maybe our model will detect many bouding box for an object, this threshold help us take the box with equal \n",
    "    #            or higher propability\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "    '''\n",
    "    out is a 2D tensor like (number_of_objects, score_of_each_classes), with first five element in each row is special, \n",
    "    take e.g: out[0] = temp:\n",
    "        + temp[0]: x_center of that object\n",
    "        + temp[1]: y_center of that object\n",
    "        + temp[2]: width of that object\n",
    "        + temp[3]: height of that object\n",
    "        + temp[4]: unknow value\n",
    "        + from 5 to above is the score of that object to each classes => COCO have 80 class so each row contain 85 element,\n",
    "            will be 15 with CIFAR,and 1005 with IMAGENET  \n",
    "    '''\n",
    "    centroid = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            # get the highest score to determine its label\n",
    "            class_id = np.argmax(scores)\n",
    "            if class_id not in [0,1,2,3,7]:\n",
    "                continue\n",
    "            else:\n",
    "                # score of that object, make sure more than 50% correct label\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # scale again with w and h\n",
    "                    center_x = int(detection[0] * Width)\n",
    "                    center_y = int(detection[1] * Height)\n",
    "                    centroid.append(np.array([center_x,center_y]))\n",
    "    lineThickness = 2\n",
    "    x1,y1,x2,y2 = (322,580,1030,580)\n",
    "    image = cv2.line(image, (x1, y1), (x2, y2), (0,255,0), lineThickness)\n",
    "    return centroid, image\n",
    "\n",
    "config = 'YOLOv3-320\\\\yolov3.cfg'\n",
    "name = 'coco.names'\n",
    "weight = 'YOLOv3-320\\\\yolov3.weights'\n",
    "\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "classes = None\n",
    "\n",
    "with open(name, 'r') as f:\n",
    "    # generate all classes of COCO, bicycle ind = 1, car ind = 2 and motorbike ind = 3\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# Read the model\n",
    "net = cv2.dnn.readNet(weight, config)\n",
    "\n",
    "# take shape of image in order to scale it to 416x416, first layer of Yolo CNN\n",
    "\n",
    "scale = 0.00392\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture('C:\\\\Users\\\\Personal Computer\\\\Desktop\\\\front.avi')\n",
    "\n",
    "count = 0\n",
    "ID_count = 0\n",
    "c_dict = {}\n",
    "while(True):\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    # Take picture for every thresh frames\n",
    "    thresh_fps = 15\n",
    "    # Take the smaller distance from this thresh to decide the ID\n",
    "    thresh_dist = 100\n",
    "    # First frame, only take position and give ID to each vehicle\n",
    "    if count == 0:\n",
    "        all_centroid, _ = find_centroid(net, frame, scale, (416,416))\n",
    "        for centroid in all_centroid:\n",
    "            c_dict[ID_count] = centroid\n",
    "            ID_count += 1\n",
    "        count += 1\n",
    "    # from second frame to above, take the previous data for tracking\n",
    "    if count%thresh_fps==0:\n",
    "        # find all centroid in each frame \n",
    "        temp,img = find_centroid(net, frame, scale, (416,416))\n",
    "        # Calculate Euclid distance from new centroid to old centroid\n",
    "        for  centroid in temp:\n",
    "            # centroid = np.array(centroid)\n",
    "            old_centroid = np.array(all_centroid)\n",
    "            dists = np.sqrt(np.sum(centroid**2) - 2*old_centroid.dot(centroid.T) + np.sum(old_centroid**2,axis=1))\n",
    "            dists = list(dists)\n",
    "            # check if dists smaller than threshold\n",
    "            chosen_dists = [i for i in dists if i <= thresh_dist]\n",
    "            # if it is a new ID\n",
    "            if not chosen_dists:\n",
    "                c_dict[ID_count] = centroid\n",
    "                ID_count+=1\n",
    "            else:\n",
    "                idx = dists.index(min(chosen_dists))\n",
    "                pre_ID = [key for key,value in c_dict.items() if list(value) == list(all_centroid[idx])][0] \n",
    "                c_dict[pre_ID] = centroid\n",
    "        for key,value in c_dict.items():\n",
    "            cv2.putText(img,str(key),tuple(value), cv2.FONT_HERSHEY_SIMPLEX , 1,(255,255,255),5)\n",
    "        all_centroid = temp[:]\n",
    "        cv2.namedWindow('image',cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('image', 800,600)\n",
    "        cv2.imshow('image',img)\n",
    "    count += 1\n",
    "    if count == thresh_fps*30:\n",
    "        count = 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dicta = {'a':[1,2],'b':[3,4]}\n",
    "b = [key for key,value in dicta.items() if value == [1,2] ]\n",
    "a = np.array([1,2])\n",
    "b = np.array([2,3])\n",
    "print(list(a)==list(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def applyFilter(frame, min, max):\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    mask = cv2.inRange(frame, min, max)\n",
    "#     mask = cv2.bitwise_not(mask)\n",
    "\n",
    "    filtered_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    return cv2.cvtColor(filtered_frame, cv2.COLOR_LAB2BGR)\n",
    "img = cv2.imread('test.png')\n",
    "low = np.array([0,128,128])\n",
    "high =  np.array([10,20,120])\n",
    "frame = applyFilter(img, low,high)\n",
    "cv2.imshow('frame',frame)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = {(0, 0): 1.0, (0, 1): 0.0, (0, 2): 0.0, (1, 0): 0.0, (1, 1): 1.0, (1, 2): 0.0, (2, 0): 1.0, (2, 1): 0.0, (2, 2): 0.0}\n",
    "o = {0: -0.0, 1: -0.0, 2: -0.0, 3: -0.0, 4: -0.0, 5: -0.0, 6: -0.0, 7: -0.0, 8: -0.0, 9: -0.0}\n",
    "import numpy as np\n",
    "def dict2arr(dictionary):\n",
    "    # take all keys in dict\n",
    "    l = list(dictionary.keys())\n",
    "    # if result is 1D tensor\n",
    "    if type(l[0])== int:\n",
    "        result = np.zeros((len(dictionary)))\n",
    "    # if result is nD tensor with n > 1\n",
    "    else:\n",
    "        # take the maximum shape, then plus 1 to generate correct shape\n",
    "        shape = [i+1 for i in max(l)]\n",
    "        result = np.zeros(shape)\n",
    "    # just let the key is index and value is value of result\n",
    "    for k,v in dictionary.items():\n",
    "        result[k]=v\n",
    "    return result\n",
    "dict2arr(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a594d0c1addf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mvlc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvlc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMediaPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rtsp://123.25.190.37:554/hosts/WORKSTATION4/DeviceIpint.1/SourceEndpoint.video:0:1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\vlc.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;31m# plugin_path used on win32 and MacOS in override.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m \u001b[0mdll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugin_path\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mfind_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mVLCException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\vlc.py\u001b[0m in \u001b[0;36mfind_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# may fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mdll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[0mplugin_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\ctypes\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found"
     ]
    }
   ],
   "source": [
    "import vlc\n",
    "player=vlc.MediaPlayer(\"rtsp://123.25.190.37:554/hosts/WORKSTATION4/DeviceIpint.1/SourceEndpoint.video:0:1\")\n",
    "player.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rtsp\n",
    "client = rtsp.Client(rtsp_server_uri = \"rtsp://VNPTIT2:Vnptit2@2018@123.25.190.37:554/hosts/WORKSTATION4/DeviceIpint.1/SourceEndpoint.video:0:1\")\n",
    "client.read().show()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening C:/Users/ADMINS/Desktop/front.avi\n"
     ]
    }
   ],
   "source": [
    "from main.centroidtracker import CentroidTracker\n",
    "from main.trackableobject import TrackableObject\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import imutils\n",
    "\n",
    "def get_output_layers(net):\n",
    "    '''\n",
    "    get all output layer names: with yolov3 is yolo_82, 94 and 106\n",
    "    ''' \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "def counting_vehicle(input_path, output_path, model_cfg, name_list,\n",
    "                    model_weight, skip_frame, thresh_prop, line):\n",
    "    '''\n",
    "    Parameters:\n",
    "        + video_path: \n",
    "            . None: take frame from camera\n",
    "            . path: take frame from video\n",
    "        + output_path: \n",
    "            . None: show the result\n",
    "            . path: save into new video\n",
    "        + name_list: list labels\n",
    "        + model_cfg: model config \n",
    "        + model_weight: model weight\n",
    "        + skip_frame: number of frame that system take sample 1 time\n",
    "        + thresh_prop: the minimum score (propability) accepted to recognize an object \n",
    "    Output: None\n",
    "        + line: horizon line for couting\n",
    "    '''\n",
    "    \n",
    "    # for Stream\n",
    "    if input_path == None:\n",
    "        print(\"Starting streaming\")\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        time.sleep(2.0)\n",
    "    \n",
    "    # open video file\n",
    "    else:\n",
    "        print(\"Opening \" + input_path)\n",
    "        cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    # write down the video\n",
    "    if output_path is not None:\n",
    "        writer = True\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        outputVideo = cv2.VideoWriter(output_path, fourcc, 20, (500,281))\n",
    "    elif output_path is None:\n",
    "        writer = False\n",
    "    # model information\n",
    "    config = model_cfg\n",
    "    weight = model_weight\n",
    "    name   = name_list\n",
    "    \n",
    "    # work with Yolo, by open COCO name list\n",
    "    with open(name, 'r') as f:\n",
    "        # generate all classes of COCO, bicycle idx = 1, car idx = 2 and motorbike idx = 3\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    # make sure the color will always be the same\n",
    "    np.random.seed(11)\n",
    "    # generate color for all classes\n",
    "    COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "    # Read the model\n",
    "    net = cv2.dnn.readNet(weight, config)\n",
    "    \n",
    "    # take shape of image in order to scale it to 416x416, first layer of Yolo CNN\n",
    "#     scale = 0.00392\n",
    "    scale = 0.00212\n",
    "\n",
    "    W = None\n",
    "    H = None\n",
    "    \n",
    "    # generate new class Centroid\n",
    "    ct = CentroidTracker(maxDisappeared= 20, maxDistance=30)\n",
    "    # class contain all element used to be detected\n",
    "    trackableObjects = {}\n",
    "    \n",
    "    # totalFrames for take skip_frame, totalDown and totalUp for counting\n",
    "    totalFrames = 0\n",
    "    totalDown = 0\n",
    "    totalUp = 0\n",
    "\n",
    "    while True:\n",
    "        # Read the video\n",
    "        ret,frame = cap.read()\n",
    "        \n",
    "        # Resize width to 500 and scale height propability equal  \n",
    "        # 500/old_width (because the smaller pics the faster our model run) \n",
    "        frame = imutils.resize(frame, width = 500)\n",
    "        \n",
    "        # convert to rgb for dlib\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if W is None or H is None:\n",
    "            # take original for scale the output of YOLO\n",
    "            (H,W) = frame.shape[:2]\n",
    "            \n",
    "        status = \"Waiting\"\n",
    "        # list for contain all vehicle objects YOLO detect\n",
    "        rects = []\n",
    "        \n",
    "        # detect for all epoch\n",
    "        if totalFrames % skip_frame == 0:\n",
    "            status = \"Detecting\"\n",
    "            trackers = []\n",
    "            # resize picture to be same shape with YOLO\n",
    "            blob = cv2.dnn.blobFromImage(frame, scale, (416, 416), (0,0,0), True, crop =False)\n",
    "            net.setInput(blob)\n",
    "            '''\n",
    "            out is a 2D tensor like (number_of_objects, score_of_each_classes), with first five element in each row \n",
    "            is special, take e.g: out[0] = temp:\n",
    "               + temp[0]: x_center of that object\n",
    "               + temp[1]: y_center of that object\n",
    "               + temp[2]: width of that object\n",
    "               + temp[3]: height of that object\n",
    "               + temp[4]: unknow value\n",
    "               + from 5 to above is the score of that object to each classes => COCO have 80 class so each row contain \n",
    "                85 element, will be 15 with CIFAR,and 1005 with IMAGENET  \n",
    "            '''\n",
    "            outs = net.forward(get_output_layers(net))\n",
    "\n",
    "            for out in outs:\n",
    "                for detection in out:\n",
    "                    # take all score\n",
    "                    scores = detection[5:]\n",
    "                    # get the highest score to determine its label\n",
    "                    class_id = np.argmax(scores)\n",
    "                    # make sure we only choose vehicle\n",
    "                    if class_id not in [0,1,2,3,7]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        # score of that object, make sure more than 50% correct label\n",
    "                        confidence = scores[class_id]\n",
    "                        if confidence > thresh_prop:\n",
    "                            center_x = int(detection[0] * W)\n",
    "                            center_y = int(detection[1] * H)\n",
    "                            w = int(detection[2] * W)\n",
    "                            h = int(detection[3] * H)\n",
    "                            # remember it return x_center and y_center, not x,y, so we need to find x,y\n",
    "                            x = int(center_x - w / 2)\n",
    "                            y = int(center_y - h / 2)\n",
    "                            # create class tracker of dlib, in order to predict the next location of vehicle\n",
    "                            tracker = dlib.correlation_tracker()\n",
    "                            rect = dlib.rectangle(x, y, x+w, y+h)\n",
    "                            tracker.start_track(rgb, rect)\n",
    "                            trackers.append(tracker)\n",
    "        else:\n",
    "            for tracker in trackers:\n",
    "                status = \"Tracking\"\n",
    "                # predictiong\n",
    "                tracker.update(rgb)\n",
    "                # new location\n",
    "                pos = tracker.get_position()\n",
    "\n",
    "                # unpack the position object\n",
    "                startX = int(pos.left())\n",
    "                startY = int(pos.top())\n",
    "                endX = int(pos.right())\n",
    "                endY = int(pos.bottom())\n",
    "                rects.append((startX, startY, endX, endY))\n",
    "        # the line, solve later\n",
    "        cv2.line(frame, (0, line), (W, line), (0, 255, 255), 2)\n",
    "        #\n",
    "        objects = ct.update(rects)\n",
    "        for (objectID, centroid) in objects.items():\n",
    "            to = trackableObjects.get(objectID, None)\n",
    "\n",
    "            if to is None:\n",
    "                to = TrackableObject(objectID, centroid)\n",
    "            else:\n",
    "                y = [c[1] for c in to.centroids]       \n",
    "                direction = centroid[1] - np.mean(y)\n",
    "                to.centroids.append(centroid)\n",
    "#                 print(\"ObjectID: {}\".format(objectID))\n",
    "#                 print(\"direction: {}\".format(direction))\n",
    "                if direction > 0.000001 and not to.counted:\n",
    "                    # if the direction is negative (indicating the object\n",
    "                    # is moving up) AND the centroid is above the center\n",
    "                    # line, count the object\n",
    "                    if direction < 0 and centroid[1] < line:\n",
    "                        totalUp += 1\n",
    "                        to.counted = True\n",
    "                    elif direction > 0 and centroid[1] > line:\n",
    "                        totalDown += 1\n",
    "                        to.counted = True\n",
    "            trackableObjects[objectID] = to\n",
    "            text = \"ID {}\".format(objectID)\n",
    "            cv2.putText(frame, text, (centroid[0], centroid[1]),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (centroid[0], centroid[1]), 4, (0, 255, 0), -1)\n",
    "        info = [\n",
    "        (\"Up\", totalUp),\n",
    "        (\"Down\", totalDown),\n",
    "        (\"Status\", status),\n",
    "        ]\n",
    "\n",
    "        for (i, (k, v)) in enumerate(info):\n",
    "                text = \"{}: {}\".format(k, v)\n",
    "                cv2.putText(frame, text, (10, H - ((i * 20) + 20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        cv2.namedWindow('Frame',cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('Frame', 1000,600)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if writer:\n",
    "            outputVideo.write(frame)\n",
    "        totalFrames += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q') or not ret:\n",
    "            break\n",
    "            \n",
    "        # release the memory\n",
    "        if totalFrames == skip_frame**3:\n",
    "            totalFrames = 1\n",
    "            \n",
    "    cap.release()\n",
    "    if writer:\n",
    "        outputVideo.release()\n",
    "    cv2.destroyAllWindows()\n",
    "counting_vehicle(input_path=\"C:/Users/ADMINS/Desktop/front.avi\",output_path = None, name_list=\"coco.names\",\n",
    "                model_cfg=\"YOLOv3-608/yolov3.cfg\",model_weight=\"YOLOv3-608/yolov3.weights\", \n",
    "                skip_frame=30, thresh_prop= 0.7, line = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "cap = cv2.VideoCapture('C:/Users/ADMINS/Desktop/front.avi')\n",
    "four = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "out = cv2.VideoWriter('C:/Users/ADMINS/Desktop/out.mp4', four, 30, (1920,1080), True)\n",
    "i = 0\n",
    "while (i < 200):\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        print(error)\n",
    "        break\n",
    "    else:\n",
    "#         frame = cv2.flip(frame,0)\n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or not ret:\n",
    "        break\n",
    "    i+=1\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103.07764064 101.01980004]\n",
      " [  5.65685425   8.48528137]\n",
      " [  2.82842712   5.65685425]]\n",
      "[2 1 0]\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "a = np.array([[110,12],[3,4],[5,6]])\n",
    "b = np.array([[7,8],[9,10]])\n",
    "D = dist.cdist(a, b)\n",
    "rows = D.min(axis=1).argsort()\n",
    "cols = D.argmin(axis=1)[rows]\n",
    "t1 = (D.argmin(axis=1))\n",
    "t2 = [rows]\n",
    "\n",
    "print(D)\n",
    "print(rows)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "img= cv2.imread('human.jpg')\n",
    "(H,W) = img.shape[:2]\n",
    "config = 'YOLOv3-608/yolov3.cfg'\n",
    "weight = 'YOLOv3-608/yolov3.weights'\n",
    "name   = 'coco.names'\n",
    "\n",
    "with open(name, 'r') as f:\n",
    "    # generate all classes of COCO, bicycle idx = 1, car idx = 2 and motorbike idx = 3\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# Read the model\n",
    "net = cv2.dnn.readNet(weight, config)\n",
    "\n",
    "scale = 0.00392\n",
    "\n",
    "# W = None\n",
    "# H = None\n",
    "trackers = []\n",
    "rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "blob = cv2.dnn.blobFromImage(img, scale, (416, 416), (0,0,0), True, crop =False)\n",
    "net.setInput(blob)\n",
    "outs = net.forward(get_output_layers(net))\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        # take all score\n",
    "        scores = detection[5:]\n",
    "        # get the highest score to determine its label\n",
    "        class_id = np.argmax(scores)\n",
    "        # make sure we only choose vehicle\n",
    "        if class_id != 0:\n",
    "            continue\n",
    "        else:\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.9:\n",
    "                center_x = int(detection[0] * W)\n",
    "                center_y = int(detection[1] * H)\n",
    "                w = int(detection[2] * W)\n",
    "                h = int(detection[3] * H)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "#                 cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "\n",
    "                # create class tracker of dlib, in order to predict the next location of vehicle\n",
    "                tracker = dlib.correlation_tracker()\n",
    "                rect = dlib.rectangle(x, y, x+w, y+h)\n",
    "                tracker.start_track(rgb, rect)\n",
    "                trackers.append(tracker)\n",
    "for tracker in trackers:\n",
    "    status = \"Tracking\"\n",
    "    # predictiong\n",
    "    tracker.update(rgb)\n",
    "    # new location\n",
    "    pos = tracker.get_position()\n",
    "\n",
    "    # unpack the position object\n",
    "    startX = int(pos.left())\n",
    "    startY = int(pos.top())\n",
    "    endX = int(pos.right())\n",
    "    endY = int(pos.bottom())\n",
    "    cv2.rectangle(img, (startX,startY), (endX,endY), (0,0,255), 2)\n",
    "cv2.imshow('test',img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coutingVehicle",
   "language": "python",
   "name": "coutingvehicle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
